{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14361317",
   "metadata": {},
   "source": [
    "\n",
    "> ðŸ“ **Note**: Make sure the dataset is placed in the `data/` directory following this structure:\n",
    ">\n",
    "> ```\n",
    "> data/\n",
    "> â”œâ”€â”€ train/\n",
    "> â”‚   â”œâ”€â”€ cats/\n",
    "> â”‚   â””â”€â”€ dogs/\n",
    "> â”œâ”€â”€ validation/\n",
    "> â”‚   â”œâ”€â”€ cats/\n",
    "> â”‚   â””â”€â”€ dogs/\n",
    "> â””â”€â”€ test/\n",
    ">     â”œâ”€â”€ cats/\n",
    ">     â””â”€â”€ dogs/\n",
    "> ```\n",
    "\n",
    "This notebook trains a binary image classifier using CNNs to distinguish between cats and dogs using TensorFlow and Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42b5ef-df02-4f4e-95c6-06f04b885acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6132d2c7-d037-4fc1-beb5-4281a2705318",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5df40f0-bb2b-4501-ab05-e75197d30dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b26724d-97b9-407d-8782-eebba0e649d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66ee8ba3-c7de-478e-ad37-6d22eafdbb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training cats images: 1000\n",
      "Total training dogs images: 1000\n",
      "Total validation cats images: 500\n",
      "Total validation dogs images: 500\n",
      "Total test cats images: 500\n",
      "Total test dogs images: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Total training cats images:\", len(os.listdir(train_cats_dir)))\n",
    "print(\"Total training dogs images:\", len(os.listdir(train_dogs_dir)))\n",
    "\n",
    "print(\"Total validation cats images:\", len(os.listdir(validation_cats_dir)))\n",
    "print(\"Total validation dogs images:\", len(os.listdir(validation_dogs_dir)))\n",
    "\n",
    "print(\"Total test cats images:\", len(os.listdir(test_cats_dir)))\n",
    "print(\"Total test dogs images:\", len(os.listdir(test_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a67c537-0a0b-47d8-82d9-357c9a14e01a",
   "metadata": {},
   "source": [
    "## ðŸ” Transfer Learning and Fine-Tuning\n",
    "\n",
    "In this project, we use a **pre-trained model** to help classify images of cats and dogs. A pre-trained model has already learned how to recognize general features in images, because it was trained on a very large dataset (like ImageNet).\n",
    "\n",
    "At first, we **freeze** the pre-trained model. This means we donâ€™t change its weights, and we only train the new layers we add on top (the classifier).\n",
    "\n",
    "Then, we do something called **fine-tuning**. We **unfreeze the last few layers** of the pre-trained model and train them again. This helps the model learn better for our specific dataset.\n",
    "\n",
    "Fine-tuning improves accuracy by adjusting the higher-level features to fit our cat vs dog classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9658113-7c13-4b93-b213-9238211862eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transfer-Learning\n",
    "pre_trained_model = VGG16(input_shape=(150,150,3),# You can increase the input size to potentially improve accuracy, but it will also increase training time and require better hardware.\n",
    "                         include_top = False, \n",
    "                         weights = 'imagenet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aff6fc-5720-49c3-9c63-0d75b188d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tunning\n",
    "pre_trained_model.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "\n",
    "for layer in pre_trained_model.layers:\n",
    "    if layer.name == \"block5_conv1\":\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2bb3ad",
   "metadata": {},
   "source": [
    "## ðŸ”š Output Layer and Activation Function\n",
    "\n",
    "Since this is a **binary classification** problem (cat vs dog), we use the **`sigmoid`** activation function in the final layer of the model.\n",
    "\n",
    "The `sigmoid` function returns a value between **0 and 1**, which we can interpret as a **probability**. For example:\n",
    "\n",
    "- If the output is close to **1**, the model predicts the image is a **dog**\n",
    "- If the output is close to **0**, the model predicts the image is a **cat**\n",
    "\n",
    "This helps us decide the final class using a simple rule:\n",
    "\n",
    "```python\n",
    "if prediction > 0.5:\n",
    "    label = \"dog\"\n",
    "else:\n",
    "    label = \"cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47f179c1-86df-4469-8ba7-cd854e71abd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(pre_trained_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29c34c94-30b1-47f5-b57d-b939b202ea86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 7,079,424\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pre_trained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f45bd66d-d48f-4713-b666-4d9f895ff093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 4, 4, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               2097408   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,812,353\n",
      "Trainable params: 9,177,089\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4bc70f-211c-443a-9831-da4f06d7a707",
   "metadata": {},
   "source": [
    "## âš™ï¸ Compiling the Model\n",
    "\n",
    "Before training a deep learning model, we need to **compile** it. This step tells the model how it should learn. During compilation, we define three main things:\n",
    "\n",
    "1. **Loss Function**:  \n",
    "   This measures how far the modelâ€™s predictions are from the actual values.  \n",
    "   For binary classification, we usually use:\n",
    "   - `binary_crossentropy`\n",
    "\n",
    "2. **Optimizer**:  \n",
    "   This decides how the model updates its weights during training. It controls how fast or slow the model learns.  \n",
    "\n",
    "3. **Metrics**:  \n",
    "   These are used to evaluate the performance of the model during training and testing.  \n",
    "   For classification tasks, we usually monitor:\n",
    "   - `accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c56c89b-5c8d-4b1b-80d7-1dfb9376829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy',\n",
    "             optimizer = RMSprop(learning_rate=1e-4),\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5bf78e",
   "metadata": {},
   "source": [
    "## ðŸ“‰ Callbacks\n",
    "\n",
    "To improve model performance and training stability, we use several callbacks provided by Keras:\n",
    "\n",
    "- **EarlyStopping**: Stops training when the validation loss stops improving, helping to prevent overfitting.\n",
    "- **ModelCheckpoint**: Saves the model with the best validation performance during training.\n",
    "- **ReduceLROnPlateau**: Reduces the learning rate when the validation loss stops improving, allowing the model to converge more effectively.\n",
    "\n",
    "In this model I'm not training it with all the callbacks defined, you can try with different callbacks and use the one with the best results.\n",
    "\n",
    "These callbacks are essential tools when training deep learning models, especially when dealing with limited datasets or noisy data.\n",
    "\n",
    "\n",
    "## â“ Why Do We Use Callbacks? \n",
    "\n",
    "They can also help the model escape from **local minima** by reducing the learning rate dynamically.\n",
    "\n",
    "The **learning rate** controls how much the model updates its weights after each epoch â€” a smaller learning rate can lead to more precise learning, while a higher one speeds up training but may miss the optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2420de7d-0c55-4969-ae85-80c76ad00863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',   \n",
    "    patience=10,           \n",
    "    restore_best_weights=True  \n",
    ")\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath='best_model.h5', \n",
    "    monitor='val_loss',        \n",
    "    save_best_only=True,       \n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',       \n",
    "    factor=0.5,                \n",
    "    patience=3,                \n",
    "    min_lr=1e-6,               \n",
    "    verbose=1                  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b6198-e6d0-4e8b-b2c1-a800d7d38f13",
   "metadata": {},
   "source": [
    "## ðŸ–¼ï¸ Data Augmentation\n",
    "\n",
    "Since our dataset is relatively small, we use **data augmentation** to generate more training examples from the existing images. This helps the model generalize better and avoid overfitting.\n",
    "\n",
    "With Keras' `ImageDataGenerator`, we apply random transformations such as:\n",
    "\n",
    "- Rotation\n",
    "- Width and height shift\n",
    "- Shearing\n",
    "- Zooming\n",
    "- Horizontal and vertical flipping\n",
    "\n",
    "These transformations create slightly modified versions of the original images, making the model more robust and improving its performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3721be-a2aa-44be-9558-671c876aae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255, # Sirve para trabjar con 0 y 1\n",
    "                                   rotation_range = 40,\n",
    "                                   width_shift_range = 0.2, \n",
    "                                   height_shift_range = 0.2, \n",
    "                                   shear_range = 0.2, \n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True,\n",
    "                                   vertical_flip = True,\n",
    "                                   fill_mode = 'nearest')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c64c82bb-0c33-46f6-b6e4-8cad4f0a84d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                   batch_size = 20,\n",
    "                                                   class_mode = 'binary',\n",
    "                                                   target_size= (150,150))\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(validation_dir,\n",
    "                                                             batch_size = 20,\n",
    "                                                             class_mode = 'binary',\n",
    "                                                             target_size = (150,150))\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                   batch_size = 20,\n",
    "                                                   class_mode = 'binary',\n",
    "                                                   target_size= (150,150))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e8b25-09a3-4162-8ee1-43f84ecb2dcc",
   "metadata": {},
   "source": [
    "## ðŸ§  Model Training\n",
    "\n",
    "With the dataset prepared and callbacks configured (they helps us to prevent the overfitting), we proceed to train the model.\n",
    "\n",
    "The training is performed using the `model.fit()` method, where we provide the training and validation datasets, the number of epochs, batch size, and the callbacks defined earlier.\n",
    "\n",
    "The goal is to optimize the model's ability to classify images of cats and dogs by minimizing the validation loss and maximizing accuracy. The callbacks help in dynamically adjusting training behavior, such as stopping early if performance stagnates or adapting the learning rate when needed.\n",
    "\n",
    "The training process is visualized through plots to monitor accuracy and loss across epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36e6d20c-c19f-4c4f-8948-d068976aadd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "steps_per_epochs = train_generator.n // batch_size\n",
    "validation_steps = validation_generator.n // batch_size\n",
    "print(steps_per_epochs)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "623c88fc-7697-47bb-a773-69c7bfcc3a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 27s 172ms/step - loss: 0.5762 - accuracy: 0.6835 - val_loss: 0.3144 - val_accuracy: 0.8430\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 0.4466 - accuracy: 0.7975 - val_loss: 0.2865 - val_accuracy: 0.8950\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 0.3899 - accuracy: 0.8320 - val_loss: 0.3683 - val_accuracy: 0.8480\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 20s 194ms/step - loss: 0.3626 - accuracy: 0.8370 - val_loss: 0.2408 - val_accuracy: 0.9030\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 0.3664 - accuracy: 0.8425 - val_loss: 0.2480 - val_accuracy: 0.8920\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 17s 169ms/step - loss: 0.3297 - accuracy: 0.8655 - val_loss: 0.1957 - val_accuracy: 0.9180\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 19s 185ms/step - loss: 0.3460 - accuracy: 0.8645 - val_loss: 0.2566 - val_accuracy: 0.8990\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 18s 184ms/step - loss: 0.2929 - accuracy: 0.8705 - val_loss: 0.1995 - val_accuracy: 0.9100\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 17s 165ms/step - loss: 0.3155 - accuracy: 0.8700 - val_loss: 0.2223 - val_accuracy: 0.9040\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 0.3036 - accuracy: 0.8795 - val_loss: 0.2007 - val_accuracy: 0.9030\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 0.2817 - accuracy: 0.8930 - val_loss: 0.2417 - val_accuracy: 0.8920\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 0.2913 - accuracy: 0.8850 - val_loss: 0.1984 - val_accuracy: 0.9120\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 16s 164ms/step - loss: 0.2796 - accuracy: 0.8920 - val_loss: 0.2215 - val_accuracy: 0.9060\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 16s 157ms/step - loss: 0.2698 - accuracy: 0.8885 - val_loss: 0.2229 - val_accuracy: 0.9130\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 0.2826 - accuracy: 0.9005 - val_loss: 0.2027 - val_accuracy: 0.9170\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.2551 - accuracy: 0.9005 - val_loss: 0.3154 - val_accuracy: 0.8900\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 16s 158ms/step - loss: 0.2641 - accuracy: 0.9010 - val_loss: 0.3195 - val_accuracy: 0.9020\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 0.2377 - accuracy: 0.9155 - val_loss: 0.2782 - val_accuracy: 0.9210\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 0.2760 - accuracy: 0.9090 - val_loss: 0.2574 - val_accuracy: 0.9170\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.2360 - accuracy: 0.9120 - val_loss: 0.2663 - val_accuracy: 0.9170\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 0.2327 - accuracy: 0.9065 - val_loss: 0.2601 - val_accuracy: 0.9160\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 0.2603 - accuracy: 0.9065 - val_loss: 0.3758 - val_accuracy: 0.8880\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 17s 167ms/step - loss: 0.2679 - accuracy: 0.9045 - val_loss: 0.3022 - val_accuracy: 0.9130\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.2474 - accuracy: 0.9095 - val_loss: 0.6226 - val_accuracy: 0.9140\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 16s 158ms/step - loss: 0.2688 - accuracy: 0.9150 - val_loss: 0.2295 - val_accuracy: 0.9220\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.2461 - accuracy: 0.9060 - val_loss: 0.2576 - val_accuracy: 0.9190\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.2254 - accuracy: 0.9260 - val_loss: 0.5755 - val_accuracy: 0.8860\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 16s 157ms/step - loss: 0.2466 - accuracy: 0.9130 - val_loss: 0.2438 - val_accuracy: 0.9230\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 15s 152ms/step - loss: 0.1946 - accuracy: 0.9320 - val_loss: 0.2758 - val_accuracy: 0.9220\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 0.2048 - accuracy: 0.9260 - val_loss: 0.3260 - val_accuracy: 0.9100\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 15s 151ms/step - loss: 0.2239 - accuracy: 0.9210 - val_loss: 0.4380 - val_accuracy: 0.9180\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 0.2090 - accuracy: 0.9170 - val_loss: 0.3371 - val_accuracy: 0.9240\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 15s 152ms/step - loss: 0.2375 - accuracy: 0.9150 - val_loss: 0.2066 - val_accuracy: 0.9230\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.2179 - accuracy: 0.9170 - val_loss: 15.2447 - val_accuracy: 0.6780\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 0.2331 - accuracy: 0.9335 - val_loss: 0.3822 - val_accuracy: 0.9080\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 16s 163ms/step - loss: 0.2244 - accuracy: 0.9320 - val_loss: 0.5205 - val_accuracy: 0.8910\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 17s 171ms/step - loss: 0.2238 - accuracy: 0.9310 - val_loss: 0.3484 - val_accuracy: 0.9180\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.1935 - accuracy: 0.9425 - val_loss: 0.5449 - val_accuracy: 0.9020\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.1812 - accuracy: 0.9265 - val_loss: 0.3832 - val_accuracy: 0.9220\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 0.2406 - accuracy: 0.9270 - val_loss: 1.2544 - val_accuracy: 0.8210\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.1888 - accuracy: 0.9430 - val_loss: 0.3931 - val_accuracy: 0.9140\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 17s 171ms/step - loss: 0.2084 - accuracy: 0.9340 - val_loss: 0.4950 - val_accuracy: 0.9170\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 17s 165ms/step - loss: 0.1820 - accuracy: 0.9320 - val_loss: 0.4239 - val_accuracy: 0.9150\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 19s 185ms/step - loss: 0.1852 - accuracy: 0.9460 - val_loss: 0.5903 - val_accuracy: 0.8990\n",
      "Epoch 45/100\n",
      " 14/100 [===>..........................] - ETA: 17s - loss: 0.1483 - accuracy: 0.9429"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\n2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  MemoryError: Unable to allocate 5.15 MiB for an array with shape (20, 150, 150, 3) and data type float32\nTraceback (most recent call last):\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 830, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 956, in generator_fn\n    yield x[i]\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 65, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 222, in _get_batches_of_transformed_samples\n    batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=self.dtype)\n\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 5.15 MiB for an array with shape (20, 150, 150, 3) and data type float32\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[IteratorGetNext/_2]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED:  MemoryError: Unable to allocate 5.15 MiB for an array with shape (20, 150, 150, 3) and data type float32\nTraceback (most recent call last):\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 830, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 956, in generator_fn\n    yield x[i]\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 65, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 222, in _get_batches_of_transformed_samples\n    batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=self.dtype)\n\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 5.15 MiB for an array with shape (20, 150, 150, 3) and data type float32\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_2258]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msteps_per_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  MemoryError: Unable to allocate 5.15 MiB for an array with shape (20, 150, 150, 3) and data type float32\nTraceback (most recent call last):\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 830, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 956, in generator_fn\n    yield x[i]\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 65, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 222, in _get_batches_of_transformed_samples\n    batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=self.dtype)\n\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 5.15 MiB for an array with shape (20, 150, 150, 3) and data type float32\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[IteratorGetNext/_2]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED:  MemoryError: Unable to allocate 5.15 MiB for an array with shape (20, 150, 150, 3) and data type float32\nTraceback (most recent call last):\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 830, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 956, in generator_fn\n    yield x[i]\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 65, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"C:\\Users\\ivcho\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 222, in _get_batches_of_transformed_samples\n    batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=self.dtype)\n\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 5.15 MiB for an array with shape (20, 150, 150, 3) and data type float32\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_2258]"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator,\n",
    "                   steps_per_epoch = steps_per_epochs,\n",
    "                   epochs = 100,\n",
    "                   validation_data = validation_generator,\n",
    "                   validation_steps = validation_steps,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb149be",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Training Metrics Visualization\n",
    "\n",
    "In the following plots, you can see how the model's **accuracy** and **loss** evolved during the training process.\n",
    "\n",
    "We track two key metrics:\n",
    "\n",
    "- **Accuracy** â€“ how often the model's predictions were correct.\n",
    "- **Loss** â€“ how far the predictions were from the true labels.\n",
    "\n",
    "This two metrics are important because we can understand how well the model is learning and wehter it's overfitting and underfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc680c01",
   "metadata": {},
   "source": [
    "## ðŸ§  Detecting Overfitting and Underfitting\n",
    "\n",
    "- **Overfitting** occurs when the model performs well on the training data but poorly on the validation data.  \n",
    "  This usually happens when the training accuracy keeps increasing, but the validation accuracy stops improving or even starts to drop.\n",
    "\n",
    "  <span style=\"color:red\"><strong>The biggest problem with overfitting is that our model is memorizing the training data, but it can't generalize when we validate what the model has learned or make predictions on test data.</strong></span>\n",
    "\n",
    "\n",
    "- **Underfitting** happens when the model is too simple or hasn't trained enough to learn from the data.  \n",
    "  In this case, both training and validation accuracy remain low, and the model doesn't improve over time.\n",
    "\n",
    "By visualizing the training and validation metrics, we can identify these issues and apply techniques like regularization, data augmentation, or early stopping to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ace534-9647-4706-a7a0-5aaff4a3867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(32, 16))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, acc, 'bo-', label='Entrenamiento', markersize=10)  \n",
    "plt.plot(epochs, val_acc, 'ro-', label='ValidaciÃ³n', markersize=10)  \n",
    "plt.title('PrecisiÃ³n durante el entrenamiento y la validaciÃ³n', fontsize=24)  \n",
    "plt.xlabel('Ã‰pocas', fontsize=20) \n",
    "plt.ylabel('PrecisiÃ³n', fontsize=20)  \n",
    "plt.legend(fontsize=18) \n",
    "plt.xticks(fontsize=16)  \n",
    "plt.yticks(fontsize=16)  \n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss, 'bo-', label='Entrenamiento', markersize=10)  \n",
    "plt.plot(epochs, val_loss, 'ro-', label='ValidaciÃ³n', markersize=10)  \n",
    "plt.title('PÃ©rdida durante el entrenamiento y la validaciÃ³n', fontsize=24)  \n",
    "plt.xlabel('Ã‰pocas', fontsize=20)  \n",
    "plt.ylabel('PÃ©rdida', fontsize=20)  \n",
    "plt.legend(fontsize=18)  \n",
    "plt.xticks(fontsize=16)  \n",
    "plt.yticks(fontsize=16)  \n",
    "\n",
    "plt.tight_layout()  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c14580-12ed-48be-9ef9-f6f9b345503a",
   "metadata": {},
   "source": [
    "## ðŸ§ª Model Evaluation and Predictions\n",
    "\n",
    "After training, we evaluate how well the model performs on **unseen test data** using the `evaluate()` function. This gives us a final measurement of the model's **loss and accuracy** on data it has never seen before.\n",
    "\n",
    "We also use the model to **make predictions** on test images with the `predict()` function. These predictions are probability values (between 0 and 1) because we used the `sigmoid` activation function in the output layer.\n",
    "\n",
    "To interpret the results:\n",
    "- Values closer to **1** mean the model predicts the image is a **dog**\n",
    "- Values closer to **0** mean it predicts a **cat**\n",
    "\n",
    "We can then compare these predictions to the actual labels to check the model's performance, plot confusion matrices, or visualize some sample results to better understand the model's behavior.\n",
    "\n",
    "This step is important to verify if the model generalizes well to new data and to detect any possible issues like overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950fb40e-d2f7-4e88-a36a-266b21f648ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b22d50-a628-49cf-b591-bef589d57be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8142827-b8a9-48b0-9b04-c55f0ec94dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885235cf-1f5b-4c4f-9112-50f3e32251fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get a specific batch through an index from the test dataset generated by ImageGenerator\n",
    "images, labels = test_generator[12]  \n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(images)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Dictionary with labels names\n",
    "label_dict = {0: 'Gato', 1: 'Perro'}\n",
    "\n",
    "for i in range(len(images)):\n",
    "    plt.subplot(4, 5, i + 1) \n",
    "    plt.imshow(images[i], cmap=plt.cm.binary)\n",
    "\n",
    "    \n",
    "    pred_label = int(predictions[i] > 0.5)\n",
    "\n",
    "    \n",
    "    plt.title(f\"Real: {label_dict[int(labels[i])]}\\nPred: {label_dict[pred_label]}\")\n",
    "    plt.axis('off') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3125b3f",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Save the Model\n",
    "\n",
    "If you want to reuse this model later â€” for deployment, testing, or integration into another application â€” make sure to save it using the following command:\n",
    "\n",
    "```python\n",
    "model.save(\"your_model_name.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
